{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction - Results Summary\n",
        "\n",
        "This notebook provides a comprehensive summary of the churn prediction pipeline results.\n",
        "\n",
        "**Note**: This notebook only reads and visualizes outputs. No core logic is executed here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model metrics\n",
        "reports_dir = project_root / \"reports\"\n",
        "\n",
        "baseline_metrics_path = reports_dir / \"baseline_metrics.json\"\n",
        "tree_metrics_path = reports_dir / \"random_forest_metrics.json\"\n",
        "\n",
        "models_data = []\n",
        "\n",
        "if baseline_metrics_path.exists():\n",
        "    with open(baseline_metrics_path) as f:\n",
        "        baseline_data = json.load(f)\n",
        "    models_data.append({\n",
        "        \"model\": \"Baseline (Logistic Regression)\",\n",
        "        \"split\": \"test\",\n",
        "        **baseline_data[\"splits\"][\"test\"]\n",
        "    })\n",
        "\n",
        "if tree_metrics_path.exists():\n",
        "    with open(tree_metrics_path) as f:\n",
        "        tree_data = json.load(f)\n",
        "    models_data.append({\n",
        "        \"model\": \"Random Forest\",\n",
        "        \"split\": \"test\",\n",
        "        **tree_data[\"splits\"][\"test\"]\n",
        "    })\n",
        "\n",
        "if models_data:\n",
        "    comparison_df = pd.DataFrame(models_data)\n",
        "    print(\"Model Performance Comparison (Test Set)\")\n",
        "    print(\"=\" * 60)\n",
        "    display(comparison_df[[\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\", \"pr_auc\"]].round(4))\n",
        "else:\n",
        "    print(\"No model metrics found. Please run the training pipeline first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "if models_data:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    \n",
        "    metrics_to_plot = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\", \"pr_auc\"]\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        ax = axes[idx // 3, idx % 3]\n",
        "        values = [m[metric] for m in models_data if metric in m and m[metric] is not None]\n",
        "        labels = [m[\"model\"] for m in models_data if metric in m and m[metric] is not None]\n",
        "        \n",
        "        if values:\n",
        "            ax.bar(labels, values, alpha=0.7)\n",
        "            ax.set_title(metric.replace(\"_\", \" \").title(), fontsize=12, fontweight=\"bold\")\n",
        "            ax.set_ylabel(\"Score\")\n",
        "            ax.set_ylim(0, 1)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add value labels on bars\n",
        "            for i, v in enumerate(values):\n",
        "                ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No data to visualize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Threshold Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load threshold analysis\n",
        "threshold_analysis_path = reports_dir / \"threshold_analysis.json\"\n",
        "\n",
        "if threshold_analysis_path.exists():\n",
        "    with open(threshold_analysis_path) as f:\n",
        "        threshold_data = json.load(f)\n",
        "    \n",
        "    print(\"Threshold Analysis Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Optimal Threshold: {threshold_data['optimal_threshold']:.4f}\")\n",
        "    print(f\"\\nOptimal Threshold Metrics:\")\n",
        "    optimal_metrics = threshold_data[\"optimal_threshold_metrics\"]\n",
        "    print(f\"  Precision: {optimal_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {optimal_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1-Score: {optimal_metrics['f1']:.4f}\")\n",
        "    print(f\"  False Positives: {optimal_metrics['false_positives']}\")\n",
        "    print(f\"  False Negatives: {optimal_metrics['false_negatives']}\")\n",
        "    \n",
        "    # Create threshold analysis DataFrame\n",
        "    threshold_df = pd.DataFrame(threshold_data[\"threshold_analysis\"])\n",
        "    \n",
        "    # Plot precision-recall vs threshold\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    ax1.plot(threshold_df[\"threshold\"], threshold_df[\"precision\"], label=\"Precision\", linewidth=2)\n",
        "    ax1.plot(threshold_df[\"threshold\"], threshold_df[\"recall\"], label=\"Recall\", linewidth=2)\n",
        "    ax1.axvline(threshold_data[\"optimal_threshold\"], color=\"red\", linestyle=\"--\", label=\"Optimal\")\n",
        "    ax1.set_xlabel(\"Threshold\")\n",
        "    ax1.set_ylabel(\"Score\")\n",
        "    ax1.set_title(\"Precision and Recall vs Threshold\", fontsize=14, fontweight=\"bold\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax2.plot(threshold_df[\"threshold\"], threshold_df[\"total_cost\"], label=\"Total Cost\", linewidth=2, color=\"red\")\n",
        "    ax2.axvline(threshold_data[\"optimal_threshold\"], color=\"green\", linestyle=\"--\", label=\"Optimal\")\n",
        "    ax2.set_xlabel(\"Threshold\")\n",
        "    ax2.set_ylabel(\"Total Cost\")\n",
        "    ax2.set_title(\"Total Cost vs Threshold\", fontsize=14, fontweight=\"bold\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Threshold analysis not found. Run threshold analysis first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Business Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load business evaluation\n",
        "business_eval_path = reports_dir / \"business_evaluation_default.json\"\n",
        "\n",
        "if business_eval_path.exists():\n",
        "    with open(business_eval_path) as f:\n",
        "        business_data = json.load(f)\n",
        "    \n",
        "    print(\"Business Evaluation Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    scenario = business_data[\"scenario\"]\n",
        "    print(f\"Scenario: {scenario['name']}\")\n",
        "    print(f\"Retention Cost: ${scenario['retention_cost_per_customer']:.2f}\")\n",
        "    print(f\"Churn Loss: ${scenario['churn_loss_per_customer']:.2f}\")\n",
        "    if scenario[\"intervention_budget\"]:\n",
        "        print(f\"Budget: ${scenario['intervention_budget']:,.2f}\")\n",
        "    \n",
        "    best_threshold = business_data[\"best_threshold\"]\n",
        "    print(f\"\\nBest Threshold: {best_threshold['threshold']:.4f}\")\n",
        "    print(f\"Net Gain: ${best_threshold['net_gain']:,.2f}\")\n",
        "    print(f\"ROI: {best_threshold['roi_percent']:.2f}%\")\n",
        "    print(f\"Interventions: {best_threshold['total_interventions']:,}\")\n",
        "    print(f\"Prevented Churns: {best_threshold['prevented_churns']:,}\")\n",
        "    \n",
        "    # Comparison table\n",
        "    comparison = business_data[\"comparison\"]\n",
        "    comparison_df = pd.DataFrame([\n",
        "        {\"Strategy\": \"Baseline\", **comparison[\"baseline\"]},\n",
        "        {\"Strategy\": \"Optimized\", **comparison[\"optimized\"]},\n",
        "        {\"Strategy\": \"Incremental\", **comparison[\"incremental\"]},\n",
        "    ])\n",
        "    \n",
        "    print(\"\\nComparison Table:\")\n",
        "    display(comparison_df[[\"Strategy\", \"net_gain\", \"interventions\", \"investment\", \"roi_percent\"]].round(2))\n",
        "    \n",
        "    # Visualize comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    strategies = comparison_df[\"Strategy\"]\n",
        "    net_gains = comparison_df[\"net_gain\"]\n",
        "    rois = comparison_df[\"roi_percent\"]\n",
        "    \n",
        "    axes[0].bar(strategies, net_gains, alpha=0.7, color=[\"gray\", \"green\", \"blue\"])\n",
        "    axes[0].set_title(\"Net Gain by Strategy\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[0].set_ylabel(\"Net Gain ($)\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    for i, v in enumerate(net_gains):\n",
        "        axes[0].text(i, v, f\"${v:,.0f}\", ha=\"center\", va=\"bottom\")\n",
        "    \n",
        "    axes[1].bar(strategies, rois, alpha=0.7, color=[\"gray\", \"green\", \"blue\"])\n",
        "    axes[1].set_title(\"ROI by Strategy\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[1].set_ylabel(\"ROI (%)\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    for i, v in enumerate(rois):\n",
        "        axes[1].text(i, v, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Business evaluation not found. Run business evaluation first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if error analysis report exists\n",
        "error_report_path = reports_dir / \"error_analysis.md\"\n",
        "\n",
        "if error_report_path.exists():\n",
        "    print(\"Error Analysis Report\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nSee error_analysis.md for detailed segment analysis.\")\n",
        "    \n",
        "    # Try to load error analysis plot\n",
        "    error_plot_path = reports_dir / \"error_analysis.png\"\n",
        "    if error_plot_path.exists():\n",
        "        from IPython.display import Image\n",
        "        display(Image(str(error_plot_path)))\n",
        "    else:\n",
        "        print(\"Error analysis plot not found.\")\n",
        "else:\n",
        "    print(\"Error analysis not found. Run error analysis first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Key Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Key Conclusions\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "conclusions = []\n",
        "\n",
        "# Model performance\n",
        "if models_data:\n",
        "    best_model = max(models_data, key=lambda x: x.get(\"roc_auc\", 0) or 0)\n",
        "    conclusions.append(f\"✓ Best Model: {best_model['model']} with ROC-AUC of {best_model.get('roc_auc', 'N/A'):.4f}\")\n",
        "\n",
        "# Threshold optimization\n",
        "if threshold_analysis_path.exists():\n",
        "    with open(threshold_analysis_path) as f:\n",
        "        threshold_data = json.load(f)\n",
        "    optimal = threshold_data[\"optimal_threshold\"]\n",
        "    conclusions.append(f\"✓ Optimal Threshold: {optimal:.4f} (minimizes total cost)\")\n",
        "\n",
        "# Business impact\n",
        "if business_eval_path.exists():\n",
        "    with open(business_eval_path) as f:\n",
        "        business_data = json.load(f)\n",
        "    incremental = business_data[\"comparison\"][\"incremental\"]\n",
        "    conclusions.append(f\"✓ Incremental Net Gain: ${incremental['net_gain']:,.2f}\")\n",
        "    conclusions.append(f\"✓ Incremental ROI: {incremental['roi_percent']:.2f}%\")\n",
        "\n",
        "if conclusions:\n",
        "    for conclusion in conclusions:\n",
        "        print(conclusion)\n",
        "else:\n",
        "    print(\"Run the full pipeline to generate conclusions.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Next Steps\n",
        "\n",
        "Based on the results:\n",
        "\n",
        "1. **Model Improvement**: Consider hyperparameter tuning if not already done\n",
        "2. **Feature Engineering**: Review error analysis to identify segments needing better features\n",
        "3. **Threshold Optimization**: Adjust threshold based on business constraints\n",
        "4. **Monitoring**: Set up monitoring for model performance in production\n",
        "5. **A/B Testing**: Test the model in production with controlled experiments\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
